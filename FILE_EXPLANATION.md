# ğŸ“ File Explanation & Complete Data Flow

## ğŸ“‹ Overview

This document explains each file's purpose and the complete flow from populating data to getting LLM output.

---

## ğŸ“ File Structure & Purpose

### ğŸš€ **Core Application Files**

#### 1. **`app.py`** - FastAPI Web Server
**Purpose:** Main application entry point and HTTP server

**Key Responsibilities:**
- Creates FastAPI web application
- Sets up HTTP endpoints (GET `/`, POST `/chat`, GET `/health`)
- Initializes Pinecone on startup
- Handles user requests from the frontend
- Routes requests to the LLM chain
- Returns responses to the user

**Key Functions:**
- `startup_event()` - Initializes Pinecone when app starts
- `home()` - Serves the HTML chat interface
- `chat_endpoint()` - Receives user messages, processes them, returns AI responses

---

#### 2. **`populate_data.py`** - Data Population Script
**Purpose:** One-time script to load cricket knowledge into Pinecone

**Key Responsibilities:**
- Loads sample cricket data
- Splits data into chunks
- Converts text to embeddings
- Stores vectors in Pinecone

**Flow:**
1. Calls `init_pinecone()` to connect to Pinecone
2. Gets sample cricket data from `data_loader.py`
3. Splits data into chunks
4. Calls `add_texts()` to store in Pinecone

---

### ğŸ“¦ **Source Code Files (`src/`)**

#### 3. **`src/data_loader.py`** - Data Loading & Processing
**Purpose:** Handles loading and chunking cricket data

**Key Functions:**
- `get_sample_cricket_data()` - Returns sample cricket knowledge text
- `split_text_into_chunks()` - Splits long text into smaller chunks (1000 chars with 200 overlap)
- `load_cricket_data_from_text()` - Processes text into chunked documents
- `load_cricket_data_from_file()` - Loads data from a file

**Why chunking?**
- Pinecone works better with smaller, focused text chunks
- Overlap (200 chars) ensures context isn't lost at chunk boundaries
- Each chunk becomes a searchable vector

---

#### 4. **`src/vector_store.py`** - Pinecone Vector Database Management
**Purpose:** Manages Pinecone connection, embeddings, and vector operations

**Key Functions:**
- `init_pinecone()` - Connects to Pinecone, creates index if needed (384 dimensions)
- `get_embeddings_model()` - Loads SentenceTransformer model (`all-MiniLM-L6-v2`)
- `add_texts()` - Converts text to embeddings and stores in Pinecone
- `query_vectors()` - Searches Pinecone for similar vectors

**Key Components:**
- **Embedding Model:** `all-MiniLM-L6-v2` (converts text â†’ 384-dim vectors)
- **Pinecone Index:** Stores vectors for similarity search
- **Functions:** Add vectors, query similar vectors

---

#### 5. **`src/llm_chain.py`** - LLM Integration & RAG Pipeline
**Purpose:** Handles Gemini LLM, RAG (Retrieval-Augmented Generation), and response generation

**Key Functions:**
- `get_gemini_model()` - Initializes Google Gemini model
- `generate_response()` - Main RAG function:
  1. Queries Pinecone for relevant context
  2. Formats prompt with context + history + question
  3. Calls Gemini to generate answer
  4. Returns answer + sources
- `format_chat_history()` - Formats conversation history for prompt
- `get_conversational_chain()` - Compatibility wrapper (maintains API consistency)

**The RAG Process:**
1. **Retrieval:** Get relevant documents from Pinecone
2. **Augmentation:** Add context to the prompt
3. **Generation:** Gemini generates answer using context

---

#### 6. **`src/helper.py`** - Utility Functions
**Purpose:** Helper utilities (currently not actively used, but available)

**Functions:**
- `format_chat_history()` - Format history for LangChain (legacy)
- `extract_sources()` - Extract source metadata
- `validate_environment()` - Check required environment variables

**Note:** Some functions here may be legacy from LangChain implementation.

---

#### 7. **`src/prompt.py`** - Prompt Templates
**Purpose:** Stores prompt templates (currently not used, prompt is in `llm_chain.py`)

**Note:** This file exists but the actual prompt is hardcoded in `llm_chain.py`. Could be refactored to use templates from here.

---

### ğŸ¨ **Frontend Files**

#### 8. **`templates/index.html`** - Chat Interface
**Purpose:** HTML UI for the chatbot

**Features:**
- Chat interface with message bubbles
- JavaScript to send/receive messages
- Real-time chat updates
- Loading indicators

#### 9. **`static/style.css`** - Styling
**Purpose:** CSS styling for the chat interface

**Features:**
- Modern gradient design
- Responsive layout
- Smooth animations

---

### âš™ï¸ **Configuration Files**

#### 10. **`requirements.txt`** - Python Dependencies
**Purpose:** Lists all Python packages needed

**Key packages:**
- `fastapi` - Web framework
- `google-generativeai` - Gemini LLM
- `pinecone-client` - Pinecone SDK
- `sentence-transformers` - Embeddings
- `uvicorn` - ASGI server

#### 11. **`env_template.txt`** - Environment Variables Template
**Purpose:** Template for `.env` file with API keys

**Required variables:**
- `PINECONE_API_KEY` - Pinecone API key
- `GOOGLE_API_KEY` - Google Gemini API key
- `PINECONE_INDEX_NAME` - Index name (default: cricket-chatbot)

---

## ğŸ”„ Complete Data Flow: From Population to LLM Output

### Phase 1: Initial Setup & Data Population

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Populate Data (One-time setup)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User runs: python populate_data.py
   â”‚
   â”œâ”€â†’ populate_data.py calls init_pinecone()
   â”‚   â”‚
   â”‚   â”œâ”€â†’ vector_store.py: init_pinecone()
   â”‚   â”‚   â”œâ”€â†’ Connects to Pinecone using API key
   â”‚   â”‚   â”œâ”€â†’ Checks if index "cricket-chatbot" exists
   â”‚   â”‚   â””â”€â†’ Creates index (384 dims) if doesn't exist
   â”‚   â”‚
   â”‚   â””â”€â†’ Returns Pinecone index object
   â”‚
   â”œâ”€â†’ populate_data.py calls get_sample_cricket_data()
   â”‚   â”‚
   â”‚   â””â”€â†’ data_loader.py: get_sample_cricket_data()
   â”‚       â””â”€â†’ Returns cricket knowledge text
   â”‚
   â”œâ”€â†’ populate_data.py calls load_cricket_data_from_text()
   â”‚   â”‚
   â”‚   â””â”€â†’ data_loader.py: load_cricket_data_from_text()
   â”‚       â”œâ”€â†’ Calls split_text_into_chunks()
   â”‚       â”‚   â””â”€â†’ Splits text into chunks (1000 chars, 200 overlap)
   â”‚       â”‚
   â”‚       â””â”€â†’ Returns list of dicts: [{"text": "...", "metadata": {...}}, ...]
   â”‚
   â””â”€â†’ populate_data.py calls add_texts()
       â”‚
       â””â”€â†’ vector_store.py: add_texts()
           â”œâ”€â†’ Gets embeddings model (SentenceTransformer)
           â”œâ”€â†’ Converts each text chunk to embedding (384-dim vector)
           â”œâ”€â†’ Prepares vectors with IDs and metadata
           â””â”€â†’ Upserts to Pinecone (stores in cloud)

Result: Cricket knowledge stored as vectors in Pinecone âœ…
```

---

### Phase 2: Application Startup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Start Application                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User runs: python app.py
   â”‚
   â””â”€â†’ app.py: startup_event()
       â”‚
       â””â”€â†’ vector_store.py: init_pinecone()
           â”œâ”€â†’ Connects to Pinecone
           â””â”€â†’ Gets existing index (already created)
       
Result: FastAPI server running, Pinecone connected âœ…
```

---

### Phase 3: User Sends Question (Live Query Flow)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: User Question â†’ LLM Response                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User types question in browser: "What are cricket formats?"
   â”‚
   â”œâ”€â†’ Browser sends POST /chat to FastAPI
   â”‚   â”‚
   â”‚   â””â”€â†’ app.py: chat_endpoint()
   â”‚       â”‚
   â”‚       â”œâ”€â†’ Validates Pinecone initialized
   â”‚       â”‚
   â”‚       â”œâ”€â†’ Converts history: [[h1, a1], [h2, a2]] â†’ [(h1, a1), (h2, a2)]
   â”‚       â”‚
   â”‚       â””â”€â†’ Calls get_conversational_chain(chat_history)
   â”‚           â”‚
   â”‚           â””â”€â†’ llm_chain.py: get_conversational_chain()
   â”‚               â””â”€â†’ Returns ChainWrapper object
   â”‚
   â”œâ”€â†’ app.py calls chain.invoke({"question": "What are cricket formats?"})
   â”‚   â”‚
   â”‚   â””â”€â†’ llm_chain.py: ChainWrapper.invoke()
   â”‚       â”‚
   â”‚       â””â”€â†’ llm_chain.py: generate_response()
   â”‚           â”‚
   â”‚           â”œâ”€â†’ Step A: RETRIEVAL
   â”‚           â”‚   â””â”€â†’ vector_store.py: query_vectors()
   â”‚           â”‚       â”œâ”€â†’ Gets embeddings model
   â”‚           â”‚       â”œâ”€â†’ Converts question to embedding: "What are..." â†’ [0.23, -0.45, ...] (384 numbers)
   â”‚           â”‚       â”œâ”€â†’ Queries Pinecone: index.query(vector=embedding, top_k=3)
   â”‚           â”‚       â””â”€â†’ Returns top 3 similar documents:
   â”‚           â”‚           [
   â”‚           â”‚             {"text": "Cricket has three formats...", "metadata": {...}, "score": 0.89},
   â”‚           â”‚             {"text": "Test cricket is played...", "metadata": {...}, "score": 0.85},
   â”‚           â”‚             {"text": "ODI format uses...", "metadata": {...}, "score": 0.82}
   â”‚           â”‚           ]
   â”‚           â”‚
   â”‚           â”œâ”€â†’ Step B: CONTEXT PREPARATION
   â”‚           â”‚   â”œâ”€â†’ Extracts text from documents: context_text = "Cricket has three formats...\n\nTest cricket..."
   â”‚           â”‚   â”œâ”€â†’ Extracts sources: sources = ["cricket_knowledge_base"]
   â”‚           â”‚   â””â”€â†’ Formats chat history: history_text = "Human: ...\nAssistant: ..."
   â”‚           â”‚
   â”‚           â”œâ”€â†’ Step C: PROMPT CONSTRUCTION
   â”‚           â”‚   â””â”€â†’ Builds prompt string:
   â”‚           â”‚       """
   â”‚           â”‚       You are a helpful cricket knowledge assistant...
   â”‚           â”‚       
   â”‚           â”‚       Context:
   â”‚           â”‚       Cricket has three formats: Test (5 days), ODI (50 overs), T20 (20 overs)...
   â”‚           â”‚       
   â”‚           â”‚       Chat History:
   â”‚           â”‚       None
   â”‚           â”‚       
   â”‚           â”‚       Human: What are cricket formats?
   â”‚           â”‚       Assistant:
   â”‚           â”‚       """
   â”‚           â”‚
   â”‚           â”œâ”€â†’ Step D: GENERATION (LLM Call)
   â”‚           â”‚   â””â”€â†’ google.generativeai: model.generate_content(prompt)
   â”‚           â”‚       â”œâ”€â†’ Sends prompt to Gemini API
   â”‚           â”‚       â”œâ”€â†’ Gemini processes prompt with context
   â”‚           â”‚       â””â”€â†’ Returns: "Cricket has three main formats: 1. Test cricket..."
   â”‚           â”‚
   â”‚           â””â”€â†’ Returns: {"answer": "...", "sources": ["cricket_knowledge_base"]}
   â”‚
   â””â”€â†’ app.py formats response
       â”‚
       â””â”€â†’ Returns JSON to browser:
           {
             "response": "Cricket has three main formats: 1. Test cricket...",
             "sources": ["cricket_knowledge_base"]
           }
       
Result: User sees answer in chat interface âœ…
```

---

## ğŸ” Detailed Step-by-Step Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE DATA FLOW                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SETUP PHASE (One-time):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
populate_data.py
    â”‚
    â”œâ”€â†’ init_pinecone() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   (vector_store.py)                           â”‚
    â”‚   Creates Pinecone index (384 dims)           â”‚
    â”‚                                                â”‚
    â”œâ”€â†’ get_sample_cricket_data() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   (data_loader.py)                            â”‚
    â”‚   Returns cricket text                        â”‚
    â”‚                                                â”‚
    â”œâ”€â†’ load_cricket_data_from_text() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   (data_loader.py)                            â”‚
    â”‚   Splits into chunks                          â”‚
    â”‚                                                â”‚
    â””â”€â†’ add_texts() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        (vector_store.py)                           â”‚
        â”‚                                            â”‚
        â”œâ”€â†’ SentenceTransformer.encode()            â”‚
        â”‚   Text â†’ 384-dim vectors                  â”‚
        â”‚                                            â”‚
        â””â”€â†’ Pinecone.upsert() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            Stores vectors in cloud


QUERY PHASE (Per request):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
User Browser
    â”‚
    â””â”€â†’ POST /chat {"message": "What are formats?", "history": []}
        â”‚
        â””â”€â†’ app.py: chat_endpoint()
            â”‚
            â””â”€â†’ llm_chain.py: generate_response()
                â”‚
                â”œâ”€â†’ RETRIEVAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   vector_store.py: query_vectors()             â”‚
                â”‚   â”‚                                            â”‚
                â”‚   â”œâ”€â†’ SentenceTransformer.encode()            â”‚
                â”‚   â”‚   "What are formats?" â†’ [0.23, -0.45, ...]â”‚
                â”‚   â”‚                                            â”‚
                â”‚   â””â”€â†’ Pinecone.index.query()                  â”‚
                â”‚       Returns top 3 similar documents          â”‚
                â”‚                                                â”‚
                â”œâ”€â†’ AUGMENTATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                â”‚   Build prompt with:                           â”‚
                â”‚   - Retrieved context                          â”‚
                â”‚   - Chat history                               â”‚
                â”‚   - User question                              â”‚
                â”‚                                                â”‚
                â””â”€â†’ GENERATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    google.generativeai: generate_content()      â”‚
                    â”‚                                            â”‚
                    â””â”€â†’ Gemini API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        Returns answer
                            â”‚
                            â””â”€â†’ app.py: JSON response
                                â”‚
                                â””â”€â†’ Browser: Display answer
```

---

## ğŸ“Š Key Data Transformations

### 1. **Text â†’ Chunks**
```
Long cricket text (2000 chars)
    â†“
split_text_into_chunks()
    â†“
["Chunk 1 (chars 0-1000)", "Chunk 2 (chars 800-1800)", ...]
```

### 2. **Chunks â†’ Embeddings**
```
Text chunk: "Cricket has three formats..."
    â†“
SentenceTransformer.encode()
    â†“
Vector: [0.23, -0.45, 0.67, ..., 0.12] (384 numbers)
```

### 3. **Embeddings â†’ Pinecone Storage**
```
Vector + Metadata
    â†“
Pinecone.upsert()
    â†“
Stored in cloud index
```

### 4. **Query â†’ Similar Vectors**
```
User question: "What are formats?"
    â†“
Embedding: [0.25, -0.43, 0.69, ...]
    â†“
Pinecone.query() (cosine similarity)
    â†“
Top 3 matching documents
```

### 5. **Context + Question â†’ LLM Response**
```
Retrieved context + Question + History
    â†“
Prompt construction
    â†“
Gemini API
    â†“
Generated answer
```

---

## ğŸ¯ File Dependencies Graph

```
app.py
  â”œâ”€â†’ src/vector_store.py (init_pinecone, get_vector_store)
  â””â”€â†’ src/llm_chain.py (get_conversational_chain)

populate_data.py
  â”œâ”€â†’ src/vector_store.py (init_pinecone, add_texts)
  â””â”€â†’ src/data_loader.py (get_sample_cricket_data, load_cricket_data_from_text)

src/llm_chain.py
  â””â”€â†’ src/vector_store.py (query_vectors)

src/vector_store.py
  â””â”€â†’ (External) SentenceTransformer, Pinecone

src/data_loader.py
  â””â”€â†’ (Standalone utility functions)
```

---

## ğŸ”‘ Key Concepts

### **RAG (Retrieval-Augmented Generation)**
1. **Retrieval:** Search Pinecone for relevant context
2. **Augmentation:** Add context to prompt
3. **Generation:** LLM generates answer using context

### **Vector Embeddings**
- Text â†’ Numbers (384 dimensions)
- Similar text = Similar vectors
- Enables semantic search

### **Chunking**
- Splits long text into smaller pieces
- Overlap ensures context continuity
- Each chunk = one searchable vector

---

This completes the full flow from data population to LLM output! ğŸ‰
